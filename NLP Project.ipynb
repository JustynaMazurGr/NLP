{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZLCn8ssnZfe",
        "outputId": "0cb980cf-f079-4926-a66b-78449e83a681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem:**\n",
        "\n",
        "**Na podstawie tekstu recenzji przewidzieć jej ocenę.**\n",
        "\n",
        "Dane ze strony: https://www.kaggle.com/datasets/andrewmvd/trip-advisor-hotel-reviews\n",
        "Tripadvisor Hotel Review Dataset file, from the publication:\n",
        "Alam, M. H., Ryu, W.-J., Lee, S., 2016. Joint multi-grain topic senti- ment: modeling semantic aspects for online reviews. Information Sci- ences 339, 206–223."
      ],
      "metadata": {
        "id": "l3DRwn-Mfmlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wczytanie danych i wstępne sprawdzenie danych:"
      ],
      "metadata": {
        "id": "2X6nNiEghqWv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bMdfHti-RMjO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "f0c15dac-6fd1-486f-d10d-39084dd3a299"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Review  Rating\n",
              "0  nice hotel expensive parking got good deal sta...       4\n",
              "1  ok nothing special charge diamond member hilto...       2\n",
              "2  nice rooms not 4* experience hotel monaco seat...       3\n",
              "3  unique, great stay, wonderful time hotel monac...       5\n",
              "4  great stay great stay, went seahawk game aweso...       5"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8fbe009-b5ff-4b8c-8398-dd3227775eef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nice hotel expensive parking got good deal sta...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ok nothing special charge diamond member hilto...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>nice rooms not 4* experience hotel monaco seat...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unique, great stay, wonderful time hotel monac...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>great stay great stay, went seahawk game aweso...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8fbe009-b5ff-4b8c-8398-dd3227775eef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8fbe009-b5ff-4b8c-8398-dd3227775eef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8fbe009-b5ff-4b8c-8398-dd3227775eef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd # importowanie biblioteki związanej z odczytem danych z pliku CSV\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Projekt z NLP/tripadvisor_hotel_reviews.csv') # Wczytanie pliku csv\n",
        "\n",
        "df.head()  #wyświetlam nagłówki i pierwsze 5 wierszy żeby zobaczyć jak wyglądają dane w pliku (co tam w ogóle jest)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape   #sprawdzam ile jest wierszy i kolumn w pliku"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugTL-QQcVtlr",
        "outputId": "c07d21ad-517d-457d-e151-a200453cabbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20491, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()   #Wyświetlam informacje o kolumnach: nazwa kolumny, typ, zliczenie danych niepustych (w ten prosty sposób sprawdzę czy brakuje jakichś danych)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-evrB6YhpoO",
        "outputId": "2f75ee00-8cf5-450a-8d44-32aea957f37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 20491 entries, 0 to 20490\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Review  20491 non-null  object\n",
            " 1   Rating  20491 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 320.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()   #Wyświetlam podsumowanie statystyczne dla danych z kolumny \"Rating\". Dzięki temu w prosty sposób sprawdzę czy faktycznie oceny są tylko w skali 1-5, czy gdzieś błędnie nie jest wpisane zero lub ocena wyższa niż 5."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "xTpdn_wT8-Mi",
        "outputId": "5fe3e77b-76b0-4ffb-b338-3eb06a789550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Rating\n",
              "count  20491.000000\n",
              "mean       3.952223\n",
              "std        1.233030\n",
              "min        1.000000\n",
              "25%        3.000000\n",
              "50%        4.000000\n",
              "75%        5.000000\n",
              "max        5.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6edd5cc-2625-404b-800c-e3919f2a1147\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>20491.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.952223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.233030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6edd5cc-2625-404b-800c-e3919f2a1147')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b6edd5cc-2625-404b-800c-e3919f2a1147 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b6edd5cc-2625-404b-800c-e3919f2a1147');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Z wstępnej analizy danych wynika, że nie mam brakujących danych i nie mam błędnych danych (tzn. oceny są z zakresu od 1-5, nie ma np. jakiejś 11)."
      ],
      "metadata": {
        "id": "dcp_kiAJ8pdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Czyszczenie danych (zgodnie z zasadmi NLP):"
      ],
      "metadata": {
        "id": "I49H9NSmh3Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Najpierw sprawdzam sobie jakie są stopwords w bibliotekach NLTK i spacy, potem na ich podstawie stworzę włane stopwords:\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS #importuję stopwords z biblioteki spacy\n",
        "import nltk\n",
        "nltk.download('stopwords') #importuję zbiór danych stopwords z biblioteki NLTK\n",
        "\n",
        "stopwords_spacy = list(STOP_WORDS)      #gotowa lista stop_words z biblioteki spacy\n",
        "print(\"\\n****************************************\\n\")\n",
        "print(f\"Stop words w tej liście spacy jest {len(stopwords_spacy)}, a wyglądają następująco:\\n\\n\",stopwords_spacy)\n",
        "\n",
        "stopwords_nltk = list(nltk.corpus.stopwords.words('english'))       #gotowa lista stop_words z biblioteki nltk\n",
        "print(\"\\n****************************************\\n\")\n",
        "print(f\"Stop words w tej liście nltk jest {len(stopwords_nltk)}, a wyglądają następująco:\\n\\n\",stopwords_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GSlPuxqBymV",
        "outputId": "cddabedf-5a48-4698-9b2e-e5001aaf4587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "****************************************\n",
            "\n",
            "Stop words w tej liście spacy jest 326, a wyglądają następująco:\n",
            "\n",
            " ['of', 'part', 'still', 'whereupon', 'under', 'onto', 'then', 'that', 'thereafter', 'mostly', 'hereby', 'please', 'beside', 'therein', 'is', 'be', 'should', 'former', 'take', 'or', '‘ve', 'regarding', 'full', 'yet', 'they', 'elsewhere', 'after', 'among', 'just', 'much', 'moreover', 'namely', 'some', 'ca', 'anyone', 'say', 'top', 'fifteen', 'toward', 'even', 'between', 'he', 'except', 'an', 'whoever', 'beyond', 'because', 'thereby', 'thence', 'through', 'also', 'rather', 'him', 'three', 'everywhere', 'the', \"'re\", 'name', 'cannot', 'she', 'these', 'sometimes', 'which', 'become', 'your', 'everything', 'whatever', 'must', 'eleven', 'used', 'almost', 'whence', 'eight', 'enough', '’d', 'forty', 'seem', 'with', 'herself', 'six', 'about', 'across', 'towards', 'was', 'it', 'amount', 'mine', 'whither', 'ten', 'when', 'go', 'in', 'yours', 'meanwhile', 'really', 'seeming', 'besides', 'were', 'amongst', 'again', 'few', 'latterly', '‘s', 'formerly', 'here', 'call', 'hereupon', 'further', 'show', '’re', 'herein', 'why', 'are', 'for', 'ours', 'such', 'neither', 'next', 'seems', 'those', 'do', 'last', 'this', \"'ve\", '’ll', 'made', 'least', '‘ll', 'had', 'himself', '’m', 'make', 'hundred', 'else', 'get', 'my', \"'m\", 'more', 'been', 'i', 'four', 'empty', 'their', 'either', 'nor', 'there', 'only', 'would', 'something', 'none', 'can', 'therefore', 'whole', 'a', 'before', 'by', 'becoming', 'may', 'might', 'am', 'to', 'his', \"'ll\", 'upon', 'various', 'n‘t', 'sixty', 'both', \"'d\", 'below', '‘m', 'will', 'whenever', 'whereas', 'has', 'fifty', 'since', 'over', 'third', 'very', 'back', 'myself', 'own', 'one', 'every', \"'s\", 'often', 'quite', 'thereupon', 'nowhere', 'too', 'perhaps', 'nine', 'via', 'becomes', 'its', 'using', 'most', 'beforehand', 'but', 'us', 'wherein', 'what', 'always', 'whom', 'due', 'our', 'many', 'keep', 'against', 'twelve', 'hereafter', 'bottom', 'no', 'however', 'whose', 'move', 'at', 'behind', \"n't\", 'several', 'somewhere', 'alone', 'as', 'indeed', 'from', 'yourselves', 'than', 'anyhow', 'although', 'noone', 're', 'ever', '’s', 'unless', 'each', 'nothing', 'together', 'out', 'twenty', 'thru', 'whether', 'ourselves', 'hence', 'afterwards', 'her', 'until', 'anyway', 'into', '’ve', 'up', 'does', 'anywhere', 'everyone', 'hers', 'being', 'around', 'less', 'so', 'where', '‘re', 'nevertheless', 'other', 'above', 'down', 'we', 'you', '‘d', 'all', 'throughout', 'me', 'sometime', 'became', 'never', 'whereby', 'yourself', 'nobody', 'if', 'doing', 'five', 'wherever', 'per', 'others', 'though', 'itself', 'how', 'first', 'see', 'anything', 'not', 'two', 'someone', 'n’t', 'any', 'did', 'along', 'seemed', 'done', 'who', 'on', 'while', 'same', 'another', 'have', 'otherwise', 'without', 'somehow', 'once', 'them', 'side', 'off', 'during', 'and', 'within', 'thus', 'give', 'latter', 'front', 'put', 'serious', 'well', 'could', 'themselves', 'now', 'already', 'whereafter']\n",
            "\n",
            "****************************************\n",
            "\n",
            "Stop words w tej liście nltk jest 179, a wyglądają następująco:\n",
            "\n",
            " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Uważam, że do oceny recenzji będą potrzebne mi przeczenia. Z biblioteki spacy ze stopwords wybieram słowa, które nie są przeczeniami i tworzę własny stopwords\n",
        "\n",
        "my_stopwords = ['th', 'see', 'whose', 'using', 'within', 'down', 'anyhow', 'seems', 'side', 'thereafter', 'five', 'in', 'become', 'take', 'because', 'upon', 'thus', 'four', 'while', '‘s', 'much', '’d', 'am', 'doing', 'whoever', 'hers', 'two', 'whereafter', 'was', 'his', 'under', 'seemed', 'besides', 'beside', \"'d\", 'at', 'again', 'thru', 'hereby', 'hereupon', 'or', 'wherein', 'twenty', 'former', '‘m', 'put', 'do', 'to', 'herein', 'off', 'used', 'whole', 'due', 'been', 'from', 'back', 'eleven', 'please', 'we', 'per', 'further', 'sometime', 'least', 'various', 'whether', 'three', 'us', 'else', 'too', 'wherever', 'same', 'other', 'first', 'own', 'one', 'were', 'them', 'anyway', 'however', 'along', 'why', 'of', 'afterwards', 'anyone', 'have', 'by', 'after', 'bottom', 'whereupon', 'where', 'this', 'between', 'really', 'may', 'well', 'sixty', 'could', 'several', 'around', 'myself', 'herself', 'up', 'already', 'quite', 'onto', 'becoming', 'name', 'him', 'more', '‘d', 'over', 'few', 'nine', 'keep', 'hereafter', 'some', 'yourself', 'serious', 'either', 'my', 'these', 'everyone', 'i', \"'ve\", 'unless', 'about', 'himself', 'latter', 'its', 'done', 'even', 'twelve', 'their', 'towards', 'might', 'although', 'always', 'throughout', 'for', 'her', 'did', 'below', '‘ve', 'there', 'seeming', 'that', 'only', 'our', 'whenever', '’m', 'those', 'ours', 'all', 'then', 'seem', 'therein', 'such', 'move', 'give', 'front', 'get', 'everything', 'others', 'whence', 'which', 'becomes', 'toward', 'as', 'whither', 'empty', \"'m\", 'though', 'another', 'the', 'what', 'they', 'show', 'once', 'somewhere', 'also', 'an', 'with', 'be', 'it', 'formerly', '’ll', 'during', 'go', 'so', 'otherwise', 'made', 'must', '’re', 'thence', 'beyond', 're', 'until', 'on', 'via', 'beforehand', 'ourselves', \"'ll\", 'a', 'sometimes', 'still', 'became', 'eight', 'last', 'make', 'would', 'meanwhile', 'next', 'yourselves', 'thereby', 'themselves', 'any', 'among', 'now', 'regarding', 'if', 'anything', 'less', 'out', 'will', 'are', 'six', 'everywhere', 'alone', 'across', 'amount', 'therefore', 'whatever', 'she', 'you', 'enough', 'through', 'behind', 'n’t', 'amongst', '’ve', 'often', 'somehow', 'forty', 'had', 'hundred', 'before', 'full', \"'re\", 'than', 'thereupon', 'each', 'and', 'yours', 'can', 'elsewhere', 'but', 'mine', 'against', 'part', 'n‘t', 'whereas', 'itself', 'top', 'rather', '’s', 'both', 'is', 'just', 'he', 'very', 'anywhere', 'indeed', 'most', 'perhaps', 'namely', 'say', 'ten', 'does', 'many', 'being', 'has', 'whom', 'above', 'hence', 'me', 'ca', 'something', 'how', 'when', 'since', 'whereby', 'who', 'almost', 'should', 'fifty', 'into', 'nevertheless', '‘ll', 'yet', '‘re', 'someone', 'together', 'mostly', 'third', 'every', 'here', \"'s\", 'latterly', 'moreover', 'fifteen', 'your', 'call', 'ever']"
      ],
      "metadata": {
        "id": "IDNu3NrRKKkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re # import biblioteki re, czyli wyrażeń regularnych, które będą użyte do oczyszczenia tesktu\n",
        "nltk.download('averaged_perceptron_tagger') #algorytm tagowania części mowy, przypisuje konkretne etykiety gramatyczne do słów w tekście, potrzebne do funkcji poniżej get_wordnet_pos\n",
        "\n",
        "# Deklaracja i definicja funkcji, która będzie czyściła dane\n",
        "def clean_text(text):\n",
        "    temp = text.lower() # Cały tekst na małe litery, aby nie było różnic\n",
        "    temp = re.sub('\\d', '', temp) # Wszystkie cyfry '\\d' zamieni na nic ''\n",
        "    temp = re.sub('<[^>]*>', '', temp)  #usunięcie tagów HTML\n",
        "    emojis = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|d|p)', temp)   #do zmiennej emojis przypisuję znaki odpowiadające emotikonom, d i p z małej bo wcześniej zmieniłam litery na małe\n",
        "    temp = re.sub('[\\W]+', ' ', temp) + ' '.join(emojis).replace('-', '')   #przemieszczam emotikony na koniec tekstu, usuwam myślnik z emotikon\n",
        "    temp = re.sub('_', '', temp) # Tutaj usuwam _\n",
        "    temp = temp.strip() # Usunięcie białych znaków na początku i końcu\n",
        "    return temp # Zwracam oczyszczony tekst\n",
        "\n",
        "\n",
        "#sprawdzam czy moja definicja funkcji czyści tekst tak jak sobie tego życzę - tzn. jeśli w tekście są emotikony to mają zostać (mogą pomóc w ocenie), pozostałe znaki będące znakami interpunkcyjnymi mają zostać usunięte\n",
        "tekst = \"He loves her, and she says that she doesn't love him :) !, so loving him is strange. She loved him once. She has 15 dogs instead, although she used to have only 1 dog. In my opinion, 1 dog is better than 15 dogs...\"\n",
        "s = clean_text(tekst)\n",
        "print(s)"
      ],
      "metadata": {
        "id": "PPVIe9oJB-iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e91d3063-a1e0-468a-a67e-9152d9947eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "he loves her and she says that she doesn t love him so loving him is strange she loved him once she has dogs instead although she used to have only dog in my opinion dog is better than dogs :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming:\n",
        "from nltk.stem.porter import PorterStemmer # Importowanie biblioteki do Porter Stemmer\n",
        "porter = PorterStemmer() # obiekt, który będzie wykorzystany do stemmingu w funkcji text_tokenizer\n",
        "\n",
        "# Lemantyzacja:\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag    #potrzebne do funkcji poniżej: get_wordnet_pos\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):  #Definicja, która zostanie zastosowana do lemantyzacji (chodzi o to, żeby lematizer nie uważał każdego słowa za rzeczownik co jest domyślne).\n",
        "    tag_dict = {\"a\": wordnet.ADJ,\n",
        "                \"n\": wordnet.NOUN,\n",
        "                \"v\": wordnet.VERB,\n",
        "                \"r\": wordnet.ADV,\n",
        "                \"s\": wordnet.ADJ_SAT}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Deklaracja funkcji, która będzie przekazywana do vectorizatora -- w celu oczyszczenia tekstu\n",
        "def text_tokenizer(text):\n",
        "    text = clean_text(text) # wywołanie wcześniej przygotowanej funkcji do oczyszczenia\n",
        "    words_after_lem = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in text.split()]   #splitem dzielę text na pojedyncze słowa oraz dokonuję lemantyzacji w celu uzyskania dokładnej formy podstawowej słowa, co pomoże w stemingu w następnym kroku (aby jak najbardziej unikać nieprawidłowych przekształceń)\n",
        "    words_after_stem = [porter.stem(word) for word in words_after_lem]  #po lemantyzacji dokonuję jeszcze stemingu co pomoże w ujednoliceniu form słów i zmniejszeniu liczby słów do analizy\n",
        "    return  [word for word in words_after_stem if word not in my_stopwords and len(word) > 2 or word == 'no' or word == 'ok']# do tego usuwam te, które są za krótkie lub są w stopwords, ale zostawiam wyrazy no oraz ok (bo mogą mieć znaczenie)\n",
        "\n",
        "# Testowanie lemantyzacji ze stemingiem:\n",
        "tekst = \"ok :-) nothing special charge diamond member hilton decided chain shot 20th anniversary seattle, start booked suite paid extra website description not, suite bedroom bathroom standard hotel room, took printed reservation desk showed said things like tv couch ect desk clerk told oh mixed suites description kimpton website sorry free breakfast, got kidding, embassy suits sitting room bathroom bedroom unlike kimpton calls suite, 5 day stay offer correct false advertising, send kimpton preferred guest website email asking failure provide suite advertised website reservation description furnished hard copy reservation printout website desk manager duty did not reply solution, send email trip guest survey did not follow email mail, guess tell concerned guest.the staff ranged indifferent not helpful, asked desk good breakfast spots neighborhood hood told no hotels, gee best breakfast spots seattle 1/2 block away convenient hotel does not know exist, arrived late night 11 pm inside run bellman busy chating cell phone help bags.prior arrival emailed hotel inform 20th anniversary half really picky wanted make sure good, got nice email saying like deliver bottle champagne chocolate covered strawberries room arrival celebrate, told needed foam pillows, arrival no champagne strawberries no foam pillows great room view alley high rise building good not better housekeeping staff cleaner room property, impressed left morning shopping room got short trips 2 hours, beds comfortable.not good ac-heat control 4 x 4 inch screen bring green shine directly eyes night, light sensitive tape controls.this not 4 start hotel clean business hotel super high rates, better chain hotels seattle,  \"\n",
        "oczyszczony_tekst=text_tokenizer(tekst)\n",
        "print(\"\\n****************************************\\n\")\n",
        "print(\"Klasyczny tekst, po podzieleniu:\")\n",
        "print(tekst.split())\n",
        "print(\"Oczyszczony tekst\")\n",
        "print(oczyszczony_tekst)\n",
        "print(\"\\n****************************************\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ1NyToUifUu",
        "outputId": "06d62b34-712d-4d23-a053-299d4d12010f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "****************************************\n",
            "\n",
            "Klasyczny tekst, po podzieleniu:\n",
            "['ok', ':-)', 'nothing', 'special', 'charge', 'diamond', 'member', 'hilton', 'decided', 'chain', 'shot', '20th', 'anniversary', 'seattle,', 'start', 'booked', 'suite', 'paid', 'extra', 'website', 'description', 'not,', 'suite', 'bedroom', 'bathroom', 'standard', 'hotel', 'room,', 'took', 'printed', 'reservation', 'desk', 'showed', 'said', 'things', 'like', 'tv', 'couch', 'ect', 'desk', 'clerk', 'told', 'oh', 'mixed', 'suites', 'description', 'kimpton', 'website', 'sorry', 'free', 'breakfast,', 'got', 'kidding,', 'embassy', 'suits', 'sitting', 'room', 'bathroom', 'bedroom', 'unlike', 'kimpton', 'calls', 'suite,', '5', 'day', 'stay', 'offer', 'correct', 'false', 'advertising,', 'send', 'kimpton', 'preferred', 'guest', 'website', 'email', 'asking', 'failure', 'provide', 'suite', 'advertised', 'website', 'reservation', 'description', 'furnished', 'hard', 'copy', 'reservation', 'printout', 'website', 'desk', 'manager', 'duty', 'did', 'not', 'reply', 'solution,', 'send', 'email', 'trip', 'guest', 'survey', 'did', 'not', 'follow', 'email', 'mail,', 'guess', 'tell', 'concerned', 'guest.the', 'staff', 'ranged', 'indifferent', 'not', 'helpful,', 'asked', 'desk', 'good', 'breakfast', 'spots', 'neighborhood', 'hood', 'told', 'no', 'hotels,', 'gee', 'best', 'breakfast', 'spots', 'seattle', '1/2', 'block', 'away', 'convenient', 'hotel', 'does', 'not', 'know', 'exist,', 'arrived', 'late', 'night', '11', 'pm', 'inside', 'run', 'bellman', 'busy', 'chating', 'cell', 'phone', 'help', 'bags.prior', 'arrival', 'emailed', 'hotel', 'inform', '20th', 'anniversary', 'half', 'really', 'picky', 'wanted', 'make', 'sure', 'good,', 'got', 'nice', 'email', 'saying', 'like', 'deliver', 'bottle', 'champagne', 'chocolate', 'covered', 'strawberries', 'room', 'arrival', 'celebrate,', 'told', 'needed', 'foam', 'pillows,', 'arrival', 'no', 'champagne', 'strawberries', 'no', 'foam', 'pillows', 'great', 'room', 'view', 'alley', 'high', 'rise', 'building', 'good', 'not', 'better', 'housekeeping', 'staff', 'cleaner', 'room', 'property,', 'impressed', 'left', 'morning', 'shopping', 'room', 'got', 'short', 'trips', '2', 'hours,', 'beds', 'comfortable.not', 'good', 'ac-heat', 'control', '4', 'x', '4', 'inch', 'screen', 'bring', 'green', 'shine', 'directly', 'eyes', 'night,', 'light', 'sensitive', 'tape', 'controls.this', 'not', '4', 'start', 'hotel', 'clean', 'business', 'hotel', 'super', 'high', 'rates,', 'better', 'chain', 'hotels', 'seattle,']\n",
            "Oczyszczony tekst\n",
            "['ok', 'noth', 'special', 'charg', 'diamond', 'member', 'hilton', 'decid', 'chain', 'shot', 'anniversari', 'seattl', 'start', 'book', 'suit', 'paid', 'extra', 'websit', 'descript', 'not', 'suit', 'bedroom', 'bathroom', 'standard', 'hotel', 'room', 'print', 'reserv', 'desk', 'thing', 'like', 'couch', 'ect', 'desk', 'clerk', 'told', 'mix', 'suit', 'descript', 'kimpton', 'websit', 'sorri', 'free', 'breakfast', 'kid', 'embassi', 'suit', 'sit', 'room', 'bathroom', 'bedroom', 'unlik', 'kimpton', 'suit', 'day', 'stay', 'offer', 'correct', 'fals', 'advertis', 'send', 'kimpton', 'prefer', 'guest', 'websit', 'email', 'ask', 'failur', 'provid', 'suit', 'advertis', 'websit', 'reserv', 'descript', 'furnish', 'hard', 'copi', 'reserv', 'printout', 'websit', 'desk', 'manag', 'duti', 'not', 'repli', 'solut', 'send', 'email', 'trip', 'guest', 'survey', 'not', 'follow', 'email', 'mail', 'guess', 'tell', 'concern', 'guest', 'staff', 'rang', 'indiffer', 'not', 'help', 'ask', 'desk', 'good', 'breakfast', 'spot', 'neighborhood', 'hood', 'told', 'no', 'hotel', 'gee', 'best', 'breakfast', 'spot', 'seattl', 'block', 'away', 'conveni', 'hotel', 'not', 'know', 'exist', 'arriv', 'late', 'night', 'insid', 'run', 'bellman', 'busi', 'chat', 'cell', 'phone', 'help', 'bag', 'prior', 'arriv', 'email', 'hotel', 'inform', 'anniversari', 'half', 'realli', 'picki', 'want', 'sure', 'good', 'nice', 'email', 'like', 'deliv', 'bottl', 'champagn', 'chocol', 'cover', 'strawberri', 'room', 'arriv', 'celebr', 'told', 'need', 'foam', 'pillow', 'arriv', 'no', 'champagn', 'strawberri', 'no', 'foam', 'pillow', 'great', 'room', 'view', 'alley', 'high', 'rise', 'build', 'good', 'not', 'housekeep', 'staff', 'cleaner', 'room', 'properti', 'impress', 'left', 'morn', 'shop', 'room', 'short', 'trip', 'hour', 'bed', 'comfort', 'not', 'good', 'heat', 'control', 'inch', 'screen', 'bring', 'green', 'shine', 'directli', 'eye', 'night', 'light', 'sensit', 'tape', 'control', 'thi', 'not', 'start', 'hotel', 'clean', 'busi', 'hotel', 'super', 'high', 'rate', 'chain', 'hotel', 'seattl']\n",
            "\n",
            "****************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deklaracja i definicja funkcji, w której będziemy przetwarzać i dzielić dane"
      ],
      "metadata": {
        "id": "uy2vZG3jpagk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_and_vectorize_text(vectorizer, X, y, test_size = 0.3):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size)\n",
        "\n",
        "  X_train_transform = vectorizer.fit_transform(X_train) #fit_transform koduje wartości.\n",
        "  X_test_transform = vectorizer.transform(X_test)\n",
        "  print('Wielkość danych po przetworzeniu:', X_train_transform.shape) \n",
        "  return X_train_transform, X_test_transform, y_train, y_test"
      ],
      "metadata": {
        "id": "zbRtxH59Co38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ustawienie metody do wektoryzacji tekstu, zdefiniowanie obiektu do wektoryrazji tekstu, utworzenie zbiorów: uczącego i testowego."
      ],
      "metadata": {
        "id": "D_9Oo1T_ppw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importujemy biblioteki z tym związane:\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF\n",
        "method = \"TF-IDF\" # wybieram metodę TF-IDF\n",
        "\n",
        "test_size = 0.3 # Ustalam wielkość (procenty) zbioru uczącego\n",
        "\n",
        "# Tworzenie wektoryzera, według ustalonej metody\n",
        "vectorizer = TfidfVectorizer(tokenizer=text_tokenizer)\n",
        "\n",
        "# Wywołujemy zdefiniowaną wcześniej funkcję\n",
        "X_train, X_test, y_train, y_test = split_and_vectorize_text(vectorizer, df['Review'], df['Rating'], test_size)  # Tutaj już mamy atrybuty warunkowe do trenowania, testowania, atrybut decyzyjny dla zbioru trenującego i testowego."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwMCelP_Cx2c",
        "outputId": "ee91147c-5b85-47fb-d62c-3a34e0f24e7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wielkość danych po przetworzeniu: (14343, 29898)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uczenie maszynowe"
      ],
      "metadata": {
        "id": "biv61zW-wnRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importowanie bibliotek do uczenia maszynowego i do oceny klasyfikacji\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# Wybrane klasyfikatory:\n",
        "classifier = {\n",
        "    'clf_SVC' : SVC(class_weight='balanced', break_ties=True),\n",
        "    'clf_LR_mod' : LogisticRegression(fit_intercept=False, class_weight='balanced', solver = 'liblinear', max_iter=100000),\n",
        "    'clf_LR' : LogisticRegression(max_iter=100000),\n",
        "}\n",
        "\n",
        "for name in classifier:\n",
        "    classifier[name].fit(X_train, y_train)\n",
        "    y_pred = classifier[name].predict(X_test) \n",
        "\n",
        "# Teraz przechodzę do walidacji mojego algorytmu (ocenę jego jakości).\n",
        "    report = classification_report(y_test,y_pred)\n",
        "    print(\"*\"*10)\n",
        "    print(name)\n",
        "    print(\"*\"*10)\n",
        "    print(report)\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfui6TnbDIO8",
        "outputId": "7a2d178a-c003-475c-ad19-3ea1f893507a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "clf_SVC\n",
            "**********\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.72      0.61      0.66       428\n",
            "           2       0.45      0.49      0.47       518\n",
            "           3       0.44      0.38      0.40       643\n",
            "           4       0.53      0.61      0.56      1805\n",
            "           5       0.78      0.73      0.76      2754\n",
            "\n",
            "    accuracy                           0.63      6148\n",
            "   macro avg       0.58      0.56      0.57      6148\n",
            "weighted avg       0.64      0.63      0.63      6148\n",
            "\n",
            "[[ 261  135   13    6   13]\n",
            " [  83  253  111   49   22]\n",
            " [  11  124  243  233   32]\n",
            " [   3   40  164 1094  504]\n",
            " [   2   12   27  689 2024]]\n",
            "**********\n",
            "clf_LR_mod\n",
            "**********\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.64      0.67      0.65       428\n",
            "           2       0.42      0.45      0.43       518\n",
            "           3       0.41      0.38      0.39       643\n",
            "           4       0.54      0.47      0.50      1805\n",
            "           5       0.74      0.80      0.77      2754\n",
            "\n",
            "    accuracy                           0.62      6148\n",
            "   macro avg       0.55      0.55      0.55      6148\n",
            "weighted avg       0.61      0.62      0.61      6148\n",
            "\n",
            "[[ 285  117   14    3    9]\n",
            " [ 117  233  107   33   28]\n",
            " [  30  123  243  195   52]\n",
            " [  11   63  187  851  693]\n",
            " [   5   22   44  485 2198]]\n",
            "**********\n",
            "clf_LR\n",
            "**********\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.71      0.62      0.66       428\n",
            "           2       0.47      0.39      0.43       518\n",
            "           3       0.48      0.25      0.33       643\n",
            "           4       0.52      0.51      0.52      1805\n",
            "           5       0.71      0.83      0.76      2754\n",
            "\n",
            "    accuracy                           0.62      6148\n",
            "   macro avg       0.58      0.52      0.54      6148\n",
            "weighted avg       0.61      0.62      0.61      6148\n",
            "\n",
            "[[ 264  120   10    7   27]\n",
            " [  86  204   86   92   50]\n",
            " [  15   80  161  297   90]\n",
            " [   6   24   70  925  780]\n",
            " [   1    4   10  458 2281]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uważam, że dla problemu przewidywania oceny, najważniejsza jest precyzja.\n",
        "\n",
        "Najlepszymi klasyfikatoromi okazały się LogisticRegression (clf_LR) oraz SVC. Dokładność kolejno 62 i 63. Wyniki mają bardzo zbliżone. Obydwa klasyfikatory słabo radzą sobie z ocenami średnimi 2-4, natomiast dobrze radzą sobie z ocenami 1 i 5. Klasyfikator SVC lepiej radzi sobie z ocenami 1 i 5 (dla ocen 1 precyzja 72, dla ocen 5 precyzja 78), natomiast dla ocen 2-4 trochę lepiej radzi sobie klasyfikator LogisticRegression.\n",
        "\n",
        "Gdyby zrobić skalę 3 - stopniową i oceny 1-2 przypisać do jednej oceny równej 1 (czyli niedostatecznie), ocenę 3 przypisać jako 2 (czyli dostatecznie), a oceny 4 i 5 do jednej oceny równej 3 (czyli bardzo dobry), wówczas najprawdopodobniej klasyfikacja dałaby bardzo dobre wyniki, ale chciałam zachować pierwotny system ocen (czyli skalę 1-5)."
      ],
      "metadata": {
        "id": "TfA3Izne0LNb"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}